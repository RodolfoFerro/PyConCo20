{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RodolfoFerro/PyConCo20/blob/full-code/notebooks/Deep%20Learning%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_qiEBgE139V"
   },
   "source": [
    "# Exploring the data\n",
    "\n",
    "This first section contains the utility functions to load the dataset to be used for our model training.\n",
    "\n",
    "The dataset used for this project is the one published in the \"[Challenges in Representation Learning: Facial Expression Recognition Challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\" by Kaggle.\n",
    "\n",
    "> **\"Challenges in Representation Learning: A report on three machine learning contests.\"** I. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. H. Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis, J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra, J. Xie, L. Romaszko, B. Xu, Z. Chuang, and Y. Bengio. arXiv 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7Wl9ToM139Z"
   },
   "source": [
    "## Data description (as detailed by Kaggle)\n",
    "\n",
    "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "\n",
    "The csv file contains two main columns, \"emotion\" and \"pixels\". The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column.\n",
    "\n",
    "This dataset was prepared by Pierre-Luc Carrier and Aaron Courville, as part of an ongoing research project. They have graciously provided the workshop organizers with a preliminary version of their dataset to use for this contest.\n",
    "\n",
    "First we need to be sure that we are using the laest version of TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuteZy-x2V53"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.0.0 tensorboard==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1lBFXpa1394"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UU7cY7wL139a"
   },
   "source": [
    "## The dataset loader\n",
    "\n",
    "This function will allow us to load directly all the dataset using pandas. This will also split the dataset into training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL-JB1oz139d"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dataset(net=True):\n",
    "    \"\"\"Utility function to load the FER2013 dataset.\n",
    "    \n",
    "    It returns the formated tuples (X_train, y_train) , (X_test, y_test).\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    net : boolean\n",
    "        This parameter is used to reshape the data from images in \n",
    "        (cols, rows, channels) format. In case that it is False, a standard\n",
    "        format (cols, rows) is used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and filter in Training/not Training data:\n",
    "    df = pd.read_csv('../data/fer2013.csv')\n",
    "    training = df.loc[df['Usage'] == 'Training']\n",
    "    testing = df.loc[df['Usage'] != 'Training']\n",
    "\n",
    "    # X_train values:\n",
    "    X_train = training[['pixels']].values\n",
    "    X_train = [np.fromstring(e[0], dtype=int, sep=' ') for e in X_train]\n",
    "    if net:\n",
    "        X_train = [e.reshape((48, 48, 1)).astype('float32') for e in X_train]\n",
    "    else:\n",
    "        X_train = [e.reshape((48, 48)) for e in X_train]\n",
    "    X_train = np.array(X_train)\n",
    "\n",
    "    # X_test values:\n",
    "    X_test = testing[['pixels']].values\n",
    "    X_test = [np.fromstring(e[0], dtype=int, sep=' ') for e in X_test]\n",
    "    if net:\n",
    "        X_test = [e.reshape((48, 48, 1)).astype('float32') for e in X_test]\n",
    "    else:\n",
    "        X_test = [e.reshape((48, 48)) for e in X_test]\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    # y_train values:\n",
    "    y_train = training[['emotion']].values\n",
    "    y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "    # y_test values\n",
    "    y_test = testing[['emotion']].values\n",
    "    y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "    return (X_train, y_train) , (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59mL7DzN139i"
   },
   "source": [
    "## Data loading\n",
    "\n",
    "If you have not downloaded the dataset yet, run the following cell. If you have already done this, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjPGpD0X139j"
   },
   "outputs": [],
   "source": [
    "!mkdir ../data\n",
    "!wget -O ../data/fer2013.csv https://www.dropbox.com/s/zi48lkarsg4kbry/fer2013.csv\\?dl\\=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GooWPAho139m"
   },
   "source": [
    "We can now use our main function to load the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxjpLkjZ139n"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vj-e6Ew4139r"
   },
   "source": [
    "We can access the data and plot some samples to check out waht's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9v3fuYQb139s"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(X_train[i].reshape((48, 48)), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3gjIRnx139x"
   },
   "source": [
    "And, to understand how each image is loaded, we can print any element as a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEgSq4CR139z"
   },
   "outputs": [],
   "source": [
    "X_train[i].reshape((48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwZBbOHX1392"
   },
   "source": [
    "# Creating our Deep Learning model\n",
    "\n",
    "Now the interesting part comes to us. Let's build a custom DL model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6Hpc9td1393"
   },
   "source": [
    "## Model architecture\n",
    "\n",
    "After some research in the state of the art for Facial Expression Recognition, I found that a simple convolutional architecture based on LeNet-5 would achieve nice results. \n",
    "\n",
    "Anyway, more recent proposals have achieved more accurate results, and even if Tensorflow already includes prebuilt models (such as MobileNet, which is one of the best model architectures for portable devices), I came up with my own implementation based on a neetwrk architecture which is supposed to be a deep-lightweight accurate model for the FER problem: \"[Extended deep neural network for facial emotion recognition (EDNN)](https://www.sciencedirect.com/science/article/abs/pii/S016786551930008X)\" by Deepak Kumar Jaina, Pourya Shamsolmoalib, and Paramjit Sehdev (Elsevier – Pattern Recognition Letters 2019).\n",
    "\n",
    "In their paper, they assure through some tests that their EDNN gives better results in classification tasks for Facial Expression Recognition, and by the architecture metrics this network turns out to be a more lightweight model compared with others.\n",
    "\n",
    "In any case, I proceeded to use Tensorflow 2.0 to build my own EDNN implementation with the Keras module.\n",
    "\n",
    "The implementation will come from two functions:\n",
    "- One to build the Residual Block\n",
    "- The second one to build the rest of the model\n",
    "\n",
    "The residual block architecture is as follows:\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://raw.githubusercontent.com/RodolfoFerro/PyConCo20/full-code/media/residual_block.png\" width=\"25%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1TUPR6t1398"
   },
   "source": [
    "We can now proceed to build our EDNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmjgIGFM1399"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def ResidualBlock(prev_layer):\n",
    "    \"\"\"Residual block from the EDNN model for FER by Deepak Kumar Jaina,\n",
    "    Pourya Shamsolmoalib & Paramjit Sehdev, as it appears in \"Extended \n",
    "    deep neural network for facial emotion recognition\", 2019.\n",
    "    \"\"\"\n",
    "    \n",
    "    # conv_1 <- Conv2D(64, (1, 1)) on prev_layer\n",
    "    # conv_2 <- Conv2D(64, (3, 3), padding=\"same\") on conv_1\n",
    "    shortc = concatenate([conv_1, conv_2], axis=-1)\n",
    "    # conv_3 <- Conv2D(128, (3, 3), padding=\"same\") on shortc\n",
    "    # conv_4 <- Conv2D(256, (1, 1)) on conv_3\n",
    "    output = concatenate([conv_4, prev_layer], axis=-1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def EDNN(n_classes=7):\n",
    "    \"\"\"\n",
    "    EDNN model for FER by Deepak Kumar Jaina, Pourya Shamsolmoalib &\n",
    "    Paramjit Sehdev, as it appears in \"Extended deep neural network for \n",
    "    facial emotion recognition\", 2019.\n",
    "    \"\"\"\n",
    "\n",
    "    x = Input(shape=(48, 48, 1))\n",
    "    y = Conv2D(32, (5, 5), input_shape=(48, 48, 1), strides=(2, 2), \n",
    "               data_format='channels_last')(x)\n",
    "    # y <- MaxPooling2D(pool_size=(2, 2)) on y\n",
    "    # y <- Conv2D(64, (3, 3), strides=(1, 1)) on y\n",
    "    # y <-  ResidualBlock on y\n",
    "    # y <- Conv2D(128, (3, 3), strides=(1, 1), padding=\"same\") on y\n",
    "    # y <- MaxPooling2D(pool_size=(2, 2)) on y\n",
    "    # y <- Conv2D(128, (3, 3), strides=(1, 1)) on y\n",
    "    # y <- ResidualBlock on y\n",
    "    # y <- Conv2D(256, (3, 3), strides=(1, 1), padding=\"same\") on y\n",
    "    # y <- MaxPooling2D(pool_size=(2, 2)) on y\n",
    "    # y <- Conv2D(512, (3, 3), strides=(1, 1), padding=\"same\") on y\n",
    "    # y <- Flatten() on y\n",
    "    # y <- Dense(1024, activation='relu') on y\n",
    "    # y <- Dropout(0.2) on y\n",
    "    # y <- Dense(512, activation='relu') on y\n",
    "    # y <- Dropout(0.2) on y\n",
    "    # y <- Dense(n_classes, activation='softmax') on y\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(x, y)\n",
    "\n",
    "    # Compile model\n",
    "    opt = SGD(lr=LRATE, momentum=0.9, decay=LRATE/EPOCHS)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiJlZBRt13-B"
   },
   "source": [
    "We now create an instance of our model and verify the details of the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krUkgf_R13-C"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "EPOCHS = 1\n",
    "BATCH = 64\n",
    "LRATE = 1e-4\n",
    "\n",
    "# Instance the model\n",
    "ednn = EDNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQPAEfGo13-H"
   },
   "outputs": [],
   "source": [
    "ednn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpT6c7Ht13-L"
   },
   "source": [
    "## Model training\n",
    "\n",
    "So far we have created out model and we already have loaded the datset. But beofr, let's create our media folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BXz14wy77HS"
   },
   "outputs": [],
   "source": [
    "!mkdir ../media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZ8iD3QM7n3b"
   },
   "source": [
    "We can now proceed to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJDgVy9r13-M"
   },
   "outputs": [],
   "source": [
    "history = ednn.fit(X_train, y_train,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   epochs=EPOCHS, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPqu0fhd13-P"
   },
   "source": [
    "After the training, we can plot the history of this process.\n",
    "\n",
    "The following functions will allow us to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1bG8vgi13-R"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(\"Model's training loss\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(\"Model's training accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_WO5WLA13-V"
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plot_loss(history)\n",
    "# plt.savefig('../media/loss.png', dpi=300)\n",
    "\n",
    "# Plot accuracy\n",
    "plot_accuracy(history)\n",
    "# plt.savefig('../media/accuracy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVCjGBb08yPp"
   },
   "source": [
    "A way to explore accuracy through classes is using a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_5pIt3m13-Y"
   },
   "outputs": [],
   "source": [
    "# Create emotions map\n",
    "emotion_labels = [\n",
    "    'Angry',\n",
    "    'Disgust',\n",
    "    'Fear',\n",
    "    'Happy',\n",
    "    'Sad',\n",
    "    'Surprise',\n",
    "    'Neutral'\n",
    "]\n",
    "\n",
    "# Predict using trained model\n",
    "y_pred = ednn.predict(X_test)\n",
    "y_pred = np.asarray([np.argmax(e) for e in y_pred])\n",
    "y_true = np.asarray([np.argmax(e) for e in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ztasu0w84B_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.set(font_scale=1.5) \n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax = sns.heatmap(cm_normalised, annot=True, linewidths=0, square=False, \n",
    "                 cmap='gray', yticklabels=emotion_labels,\n",
    "                 xticklabels=emotion_labels, vmin=0,\n",
    "                 vmax=np.max(cm_normalised), fmt=\".2f\",\n",
    "                 annot_kws={\"size\": 20})\n",
    "ax.set(xlabel='Predicted label', ylabel='True label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu1sXL019CLN"
   },
   "source": [
    "## Saving the trained model\n",
    "\n",
    "To save the trained model we will basically do two things:\n",
    "\n",
    "- Serialize the model into a JSON file, which will save the architecture of our model.\n",
    "- Serialize the weights into a HDF5 file, which will save all parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xy0H-d419DeD"
   },
   "outputs": [],
   "source": [
    "# Serialize model to JSON\n",
    "json_ednn = ednn.to_json()\n",
    "with open('../data/model.json', 'w') as json_file:\n",
    "    json_file.write(json_ednn)\n",
    "\n",
    "# Serialize weights to HDF5 (h5py needed)\n",
    "ednn.save_weights('../data/model.h5')\n",
    "print('Model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOECl5Rl9N53"
   },
   "source": [
    "> ### ❗️ If youre using Google Colab only!\n",
    "\n",
    "## Downloading the saved model\n",
    "\n",
    "We just need to import the Google Colab module and download the specified files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLt12vOb9jTy"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "model_files = ['../data/model.json', '../data/model.h5']\n",
    "for file in model_files:\n",
    "    files.download(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILdHHOFwFXQ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Deep Learning Model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
